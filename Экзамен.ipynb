{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamkamzer/pythonProject1/blob/main/%D0%AD%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y1SHnawu0e0"
      },
      "source": [
        "# Импорт библиотек и загрузка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2moqcC5vKSn"
      },
      "source": [
        "Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCRwrYb6vRxq",
        "outputId": "e3e196c4-badd-4760-b6f1-6fb36dfeefa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-25 15:53:46--  https://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/shakespeare.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94275 (92K) [text/plain]\n",
            "Saving to: ‘shakespeare.txt.6’\n",
            "\n",
            "\rshakespeare.txt.6     0%[                    ]       0  --.-KB/s               \rshakespeare.txt.6   100%[===================>]  92.07K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-12-25 15:53:46 (6.44 MB/s) - ‘shakespeare.txt.6’ saved [94275/94275]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/shakespeare.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOG5HhETvaVW"
      },
      "source": [
        "Импорт библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "WEw4jS1QvZnj"
      },
      "outputs": [],
      "source": [
        "# Импортируем библиотеки:\n",
        "\n",
        "# Импортация библиотеки для работы с регулярными выражениями\n",
        "import re\n",
        "# Импортация библиотеки для получения инструментов для обработки текста, а также список стоп-слов для удаления их из текста\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "# Импортация модуля string для работы с пунктуацией\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGgVy-pBjmEn",
        "outputId": "8875c505-1774-4612-96a8-a190608ffade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "# Загрузка ресурсов из NLTK (токенизатор, стоп-слова)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZmaaWA9wCbq"
      },
      "source": [
        "# Экзаменационное задание"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Mz5pTfVFyPbA"
      },
      "outputs": [],
      "source": [
        "# Выстраиваем путь к файлу, открываем его и возвращаем содержимое в виде строки\n",
        "def open_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "ABkVAtL_yGGD"
      },
      "outputs": [],
      "source": [
        "# Токенизирует текст (разделяем текст на отдельные слова)\n",
        "def tokenize_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "mc1E1lQByLqU"
      },
      "outputs": [],
      "source": [
        "# Удаляем стоп-слова из списка токенов с помощью предварительно определённого списка стоп-слов\n",
        "def remove_stopwords(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    return filtered_tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Удаляем пунктуацию из списка токенов\n",
        "def remove_punctuation(tokens):\n",
        "    no_punct = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens if word.strip()]\n",
        "    return no_punct"
      ],
      "metadata": {
        "id": "3bE-Ni24nnX_"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "2xEZ1YCazlJq"
      },
      "outputs": [],
      "source": [
        "# Открываем файл, токенизируем текст, удаляем стоп-слова, выделяем частоту слов и выводим наиболее часто встречающиеся слова\n",
        "def word_frequency_analysis(file_path, num_words):\n",
        "    text = open_file(file_path)\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "\n",
        "    filtered_words = [word for word in tokens if word.isalpha()]\n",
        "    word_count = Counter(filtered_words)\n",
        "\n",
        "    top_words = word_count.most_common(num_words)\n",
        "    return top_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZQ_ThBCzjdw",
        "outputId": "7e49b3ca-26e7-436a-f47e-9367de76fac2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('and', 490),\n",
              " ('the', 432),\n",
              " ('to', 408),\n",
              " ('my', 393),\n",
              " ('of', 370),\n",
              " ('i', 351),\n",
              " ('that', 323),\n",
              " ('in', 323),\n",
              " ('thy', 287),\n",
              " ('thou', 234)]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "file_path = 'shakespeare.txt'\n",
        "word_frequency_analysis(file_path, 10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}