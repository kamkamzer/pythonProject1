{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamkamzer/pythonProject1/blob/main/%D0%9C%D0%BE%D0%B4%D1%83%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F_%D0%B4%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашняя работа по темам первого модуля\n",
        "\n",
        "\n",
        "Курс В.И. Фирсановой \"Основы программирования на Python\", НИУ ВШЭ СПб"
      ],
      "metadata": {
        "id": "LKJXB1rGMi22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задачи"
      ],
      "metadata": {
        "id": "V7QBQXMlOw9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 1. Swap variables"
      ],
      "metadata": {
        "id": "5uvHoLGmbuWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solution(a, b):\n",
        "    \"\"\"\n",
        "    Программа принимает на вход две переменные (a, b) и меняет местами их значения\n",
        "\n",
        "    \"\"\"\n",
        "    a, b = b, a\n",
        "    return a, b\n",
        "\n",
        "test_data = [(5, 10),\n",
        "             ('cat', 'dog'),\n",
        "             ({'name': 'John', 'surname': 'Doe'}, {'name': 'John', 'surname': 'Smith'})]\n",
        "test_result = [(10, 5),\n",
        "               ('dog', 'cat'),\n",
        "               ({'name': 'John', 'surname': 'Smith'}, {'name': 'John', 'surname': 'Doe'})]\n",
        "\n",
        "result = [solution(x, y) for x, y in test_data]\n",
        "\n",
        "print(result)\n",
        "\n",
        "if result == test_result:\n",
        "    print('Correct!')\n",
        "else:\n",
        "    print('Improve')"
      ],
      "metadata": {
        "id": "wytx0o4gXavX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b91bd79-341a-448c-c3ba-bfede3419f54"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(10, 5), ('dog', 'cat'), ({'name': 'John', 'surname': 'Smith'}, {'name': 'John', 'surname': 'Doe'})]\n",
            "Correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 2. Имена с большой буквы"
      ],
      "metadata": {
        "id": "MhzTcWyjb1-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solution(name):\n",
        "    \"\"\"\n",
        "    Программа принимает на вход имена, записанные в нижнем регистре,\n",
        "    и возвращает те же имена, написанные с большой буквы\n",
        "\n",
        "    \"\"\"\n",
        "    capitalized_name = name.capitalize()\n",
        "    return capitalized_name\n",
        "\n",
        "test_data = ['john', 'mary', 'samantha']\n",
        "test_result = ['John', 'Mary', 'Samantha']\n",
        "\n",
        "result = [solution(name) for name in test_data]\n",
        "\n",
        "print(result)\n",
        "\n",
        "if result == test_result:\n",
        "    print('Correct!')\n",
        "else:\n",
        "    print('Improve')"
      ],
      "metadata": {
        "id": "DiYzztjbYtmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1763b5fc-9452-49a6-e1a6-dbd24bb699c2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['John', 'Mary', 'Samantha']\n",
            "Correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 3. Токенизатор"
      ],
      "metadata": {
        "id": "R7bUh5xgcSIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solution(sentence):\n",
        "    \"\"\"\n",
        "    Программа принимает текст, записанный в строку, приводит его к нижнему регистру,\n",
        "    разбивает текст на слова и возвращает созданный список слов\n",
        "\n",
        "    \"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    tokenized_sentence = sentence.split()\n",
        "    tokenized_sentence = sentence.lower().split()\n",
        "    return tokenized_sentence\n",
        "\n",
        "test_data = ['This a sample sentence',\n",
        "             'THIS IS A CAPITALIZED SENTENCE',\n",
        "             'and one MORE SENTENCE']\n",
        "test_result = [['this', 'a', 'sample', 'sentence'],\n",
        "               ['this', 'is', 'a', 'capitalized', 'sentence'],\n",
        "               ['and', 'one', 'more', 'sentence']]\n",
        "\n",
        "result = [solution(sentence) for sentence in test_data]\n",
        "\n",
        "print(result)\n",
        "\n",
        "if result == test_result:\n",
        "    print('Correct!')\n",
        "else:\n",
        "    print('Improve')"
      ],
      "metadata": {
        "id": "ogthnNS-Z_QA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944dbcc1-7f66-4a59-d011-a17a99e75d14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['this', 'a', 'sample', 'sentence'], ['this', 'is', 'a', 'capitalized', 'sentence'], ['and', 'one', 'more', 'sentence']]\n",
            "Correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 4. Нативный частотный словарь"
      ],
      "metadata": {
        "id": "mSArNcK2ckXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solution(text):\n",
        "    \"\"\"\n",
        "    Программа принимает текст, записанный в строку, приводит его к нижнему регистру,\n",
        "    разбивает текст на слова и создает частотный словарь получившихся токенов.\n",
        "    За один токен принимается словоформа (не нужно приводить слово к начальной форме).\n",
        "    Частотный словарь состоит из пар ключ-значение:\n",
        "    ключ - токен (слово), значение - сколько раз оно встречается в тексте.\n",
        "    Используйте цикл for и условия. Если слово еще не записано в частотный словарь,\n",
        "    внесите его в словарь со значением 1. Если слово уже записано в частотный словарь,\n",
        "    прибавьте к его значению единицу.\n",
        "\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    words = text.split()\n",
        "\n",
        "    freq_dict = {}\n",
        "\n",
        "    for word in words:\n",
        "      if word not in freq_dict:\n",
        "        freq_dict[word] = 1\n",
        "      else:\n",
        "        freq_dict[word] += 1\n",
        "\n",
        "    return freq_dict\n",
        "\n",
        "test_data = ['NLP is a subfield of AI and NLP is widely used',\n",
        "             'Count the word frequency and build a frequency dictionary',\n",
        "             'This task is not hard this task is ovewhelming this task is fun']\n",
        "test_result = [{'nlp': 2, 'is': 2, 'a': 1, 'subfield': 1, 'of': 1, 'ai': 1, 'and': 1, 'widely': 1, 'used': 1},\n",
        "               {'count': 1, 'the': 1, 'word': 1, 'frequency': 2, 'and': 1, 'build': 1, 'a': 1, 'dictionary': 1},\n",
        "               {'this': 3, 'task': 3, 'is': 3, 'not': 1, 'hard': 1, 'ovewhelming': 1, 'fun': 1}]\n",
        "\n",
        "result = [solution(text) for text in test_data]\n",
        "\n",
        "print(result)\n",
        "\n",
        "if result == test_result:\n",
        "    print('Correct!')\n",
        "else:\n",
        "    print('Improve')"
      ],
      "metadata": {
        "id": "sSFSQoUDbIL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc9b6ac6-35d3-4385-c4a0-7e1dd79fc24f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'nlp': 2, 'is': 2, 'a': 1, 'subfield': 1, 'of': 1, 'ai': 1, 'and': 1, 'widely': 1, 'used': 1}, {'count': 1, 'the': 1, 'word': 1, 'frequency': 2, 'and': 1, 'build': 1, 'a': 1, 'dictionary': 1}, {'this': 3, 'task': 3, 'is': 3, 'not': 1, 'hard': 1, 'ovewhelming': 1, 'fun': 1}]\n",
            "Correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 5. Найти уникальные словоформы"
      ],
      "metadata": {
        "id": "O68GfLhugw_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solution(text):\n",
        "    \"\"\"\n",
        "    Программа принимает текст, записанный в строку, приводит его к нижнему регистру,\n",
        "    разбивает текст на слова и возвращает список уникальных словоформ.\n",
        "    Какой тип данных помогает отфильтровать объект от повторов?\n",
        "\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    words = text.split()\n",
        "\n",
        "    unique_tokens = set(words)\n",
        "    return unique_tokens\n",
        "\n",
        "test_data = ['NLP is the future of AI and the future is now',\n",
        "             'Count the word frequency to build a frequency dictionary',\n",
        "             'This task is not hard this task is fun']\n",
        "test_result = [{'the', 'is', 'and', 'now', 'of', 'ai', 'nlp', 'future'},\n",
        "               {'the', 'count', 'dictionary', 'frequency', 'a', 'build', 'to', 'word'},\n",
        "               {'hard', 'is', 'task', 'not', 'this', 'fun'}]\n",
        "\n",
        "result = [solution(text) for text in test_data]\n",
        "\n",
        "print(result)\n",
        "\n",
        "if result == test_result:\n",
        "    print('Correct!')\n",
        "else:\n",
        "    print('Improve')"
      ],
      "metadata": {
        "id": "hWvEA-5pcjDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb4d0f9a-c7db-484e-e707-3d2907677bf0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'ai', 'is', 'and', 'nlp', 'future', 'now', 'the', 'of'}, {'word', 'build', 'dictionary', 'frequency', 'a', 'count', 'to', 'the'}, {'hard', 'is', 'this', 'task', 'not', 'fun'}]\n",
            "Correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 6. Список покупок"
      ],
      "metadata": {
        "id": "omYck7W4iyq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solution(price_list):\n",
        "    \"\"\"\n",
        "    Программа принимает на вход список словарь, в котором записаны наименования\n",
        "    товаров и их стоимость. Программа возвращает человекочитаемый вариант словаря\n",
        "    в виде списка строк вида \"Наименование: $0.00\".\n",
        "    Необходимо произвести итерацию по словарю, с помощью join и форматирования строк\n",
        "    составить строки вида \"Наименование: $0.00\" и сохранить все в единый список.\n",
        "\n",
        "    \"\"\"\n",
        "    items_list = []\n",
        "\n",
        "    for item, price in price_list.items():\n",
        "      item_string = f'{item}: ${price:.2f}'\n",
        "      items_list.append(item_string)\n",
        "\n",
        "    return items_list\n",
        "\n",
        "test_data = [{'Apples': 2.48, 'Bananas': 1.99, 'Bread': 3.49},\n",
        "             {'Dress': 9.78, 'Pants': 19.96},\n",
        "             {'Ball': 1.33, 'Doll': 1.99, 'Car': 3.50}]\n",
        "test_result = [['Apples: $2.48', 'Bananas: $1.99', 'Bread: $3.49'],\n",
        "               ['Dress: $9.78', 'Pants: $19.96'],\n",
        "               ['Ball: $1.33', 'Doll: $1.99', 'Car: $3.50']]\n",
        "\n",
        "result = [solution(price_list) for price_list in test_data]\n",
        "\n",
        "print(result)\n",
        "\n",
        "if result == test_result:\n",
        "    print('Correct!')\n",
        "else:\n",
        "    print('Improve')"
      ],
      "metadata": {
        "id": "sUOFTa98hgCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9b1d349-f13f-41e2-d60c-c356e81575da"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Apples: $2.48', 'Bananas: $1.99', 'Bread: $3.49'], ['Dress: $9.78', 'Pants: $19.96'], ['Ball: $1.33', 'Doll: $1.99', 'Car: $3.50']]\n",
            "Correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 7. Фильтр слов по длине"
      ],
      "metadata": {
        "id": "6a5HMo-KmjrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solution(sentence):\n",
        "    \"\"\"\n",
        "    Программа принимает на вход текст, токенизирует его и возвращает список слов,\n",
        "    длина которых не менее 3 и не более 10 символов. Используйте list comprehension.\n",
        "\n",
        "    \"\"\"\n",
        "    ### ваш код здесь ###\n",
        "\n",
        "    return filtered_words\n",
        "\n",
        "test_data = ['This is a sentence',\n",
        "             'Thissentence contains a mistake',\n",
        "             'One more sentence to test_this_code']\n",
        "test_result = [['this', 'sentence'],\n",
        "               ['contains', 'mistake'],\n",
        "               ['more', 'sentence']]\n",
        "\n",
        "result = [solution(sentence) for sentence in test_data]\n",
        "\n",
        "print(result)\n",
        "\n",
        "if result == test_result:\n",
        "    print('Correct!')\n",
        "else:\n",
        "    print('Improve')"
      ],
      "metadata": {
        "id": "l7DDGMPApcnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 8. Список гласных"
      ],
      "metadata": {
        "id": "H5Yh3eZimn93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solution(text):\n",
        "    \"\"\"\n",
        "    Программа принимает на вход текст и возвращает частотный словарь гласных,\n",
        "    которые встречаются в этом тексте. Используйте list comprehension для создания списка гласных.\n",
        "    Далее с помощью цикла for и условий создайте словарь, в котором ключ - гласная, а значение -\n",
        "    количество повторов этой гласной в тексте. Для этого произведите итерацию по созданному\n",
        "    списку гласных и обновляйте значения словаря, если текущая гласная уже записана в словаре.\n",
        "\n",
        "    \"\"\"\n",
        "    vowels = [char for char in text if char in 'AIUEOaiueo']\n",
        "\n",
        "    vowels_dict = {}\n",
        "\n",
        "    for vowel in vowels:\n",
        "      if vowel in vowels_dict:\n",
        "        vowels_dict[vowel] += 1\n",
        "      else:\n",
        "        vowels_dict[vowel] = 1\n",
        "\n",
        "    return vowels_dict\n",
        "\n",
        "test_data = ['Never gonna give you up',\n",
        "             'Never gonna let you down',\n",
        "             'Never gonna run around and desert you']\n",
        "test_result = [{'e': 3, 'o': 2, 'a': 1, 'i': 1, 'u': 2},\n",
        "               {'e': 3, 'o': 3, 'a': 1, 'u': 1},\n",
        "               {'e': 4, 'o': 3, 'a': 3, 'u': 3}]\n",
        "\n",
        "result = [solution(text) for text in test_data]\n",
        "\n",
        "print(result)\n",
        "\n",
        "if result == test_result:\n",
        "    print('Correct!')\n",
        "else:\n",
        "    print('Improve')"
      ],
      "metadata": {
        "id": "vecahGvkWjFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef93d63-e90c-4c27-8c20-e0816d7bdad4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'e': 3, 'o': 2, 'a': 1, 'i': 1, 'u': 2}, {'e': 3, 'o': 3, 'a': 1, 'u': 1}, {'e': 4, 'o': 3, 'a': 3, 'u': 3}]\n",
            "Correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 9. Удаление стоп-слов"
      ],
      "metadata": {
        "id": "mj4ttl5Lmr7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def solution(text):\n",
        "    \"\"\"\n",
        "    Программа принимает на вход текст и возвращает список слов, очищенный от стоп-слов.\n",
        "    Программа записывает в переменную список стоп-слов из nltk,\n",
        "    производит токенизацию с помощью инструмента word_tokenize из nltk,\n",
        "    создает список слов, очищенный от стоп-слов с помощью list comprehension.\n",
        "    Если слово, приведенное к нижнему регистру не обнаружено в списке stop_words,\n",
        "    это слово заносится в список слов, очищенных от стоп-слов (cleaned_text).\n",
        "\n",
        "    \"\"\"\n",
        "    # список стоп-слов из nltk\n",
        "    stop_words = stopwords.words('english')\n",
        "    # токенизация nltk\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "    cleaned_text = [i for i in words if i.lower() not in stop_words]\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "test_data = ['This is an example sentence with some stop words.',\n",
        "             'I am a tiny sentence with a lot of stop words!',\n",
        "             'Hey, wanna find some stop words?']\n",
        "test_result = [['example', 'sentence', 'stop', 'words', '.'],\n",
        "               ['tiny', 'sentence', 'lot', 'stop', 'words', '!'],\n",
        "               ['Hey', ',', 'wan', 'na', 'find', 'stop', 'words', '?']]\n",
        "\n",
        "result = [solution(text) for text in test_data]\n",
        "\n",
        "print(result)\n",
        "\n",
        "if result == test_result:\n",
        "    print('Correct!')\n",
        "else:\n",
        "    print('Improve')"
      ],
      "metadata": {
        "id": "G_w_KXhJnILw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d45db3-1b44-4078-f141-11470806ccbf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['example', 'sentence', 'stop', 'words', '.'], ['tiny', 'sentence', 'lot', 'stop', 'words', '!'], ['Hey', ',', 'wan', 'na', 'find', 'stop', 'words', '?']]\n",
            "Correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 10. Лемматизация [не получилось :(]"
      ],
      "metadata": {
        "id": "9BRBSYPanIaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def solution(text):\n",
        "    \"\"\"\n",
        "    Программа принимает на вход текст и возвращает список лемм (начальных форм слова)\n",
        "    с помощью библиотеки Spacy. Результатом работы программы является строка,\n",
        "    в которой слова записаны подряд через пробел.\n",
        "    Текст токенизируется в Spacy, далее с помощью list comprehension программа находит лемму\n",
        "    (token.lemma_) для каждого токена в документе. На месте token может стоять любая другая переменная,\n",
        "    например, i. Затем леммы из списка \"склеиваются\" в одну строку с помощью join.\n",
        "\n",
        "    \"\"\"\n",
        "    # токенизируем текст с помощью Spacy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    stop_words = stopwords.words('English')\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    cleaned_text =[i for i in words if i.lower() not in stop_words]\n",
        "\n",
        "    return lemmatized_text\n",
        "\n",
        "test_data = ['they have been to several countries',\n",
        "             'this is the last task hope you\\'re happy',\n",
        "             'happier than ever']\n",
        "test_result = ['they have be to several country',\n",
        "               'this be the last task hope you be happy',\n",
        "               'happy than ever']\n",
        "\n",
        "result = [solution(text) for text in test_data]\n",
        "\n",
        "print(result)\n",
        "\n",
        "if result == test_result:\n",
        "    print('Correct!')\n",
        "else:\n",
        "    print('Improve')"
      ],
      "metadata": {
        "id": "C7-UY9hOnKNw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "8d0a18ea-29c5-406f-f7e2-3e582eb1a4de"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-26182865f1a4>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                'happy than ever']\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-26182865f1a4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m                'happy than ever']\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-26182865f1a4>\u001b[0m in \u001b[0;36msolution\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'English'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     19\u001b[0m         return [\n\u001b[1;32m     20\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/api.py\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m                 \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/api.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \"\"\"\n\u001b[1;32m    230\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, fileid)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, _path)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such file or directory: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: No such file or directory: '/root/nltk_data/corpora/stopwords/English'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JUYeyO-GZC90"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}